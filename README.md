Abstract: The advent of social media has transformed the way we communicate and connect, enabling individuals worldwide to instantly and openly interact with friends, family, and colleagues on a frequent basis. Moreover, it has facilitated the establishment of new relationships. People utilize social media platforms as a means to express their opinions, share personal experiences, narratives, and challenges. Nevertheless, concerns have arisen due to the growing prevalence of suicidal content on social media platforms, where discussions of hardship, thoughts of death, and self-harm are widespread, particularly among younger generations. Consequently, harnessing the power of social media to detect and identify suicidal behavior, including the presence of suicidal thoughts, becomes essential in offering appropriate interventions that discourage self-harm and suicide, as well as in preventing the spread of suicidal ideations throughout these platforms. This paper present a suicidal content detection using various deep learning architecture  like RNN, GRU, LSTM, Bidirectional LSTM and DistilBERT with an accuracy of 84%, 93.20%, 93.26%,92.99% and 97.75% respectively and finally deploy the model using Telegram bot with detect the message and in suicidal case it respond give some motivation and inform their friends and relative.
Dataset: The dataset, curated by Nikhileswar Komati and publicly available on Kaggle, comprises posts obtained from the "SuicideWatch" subreddits on the Reddit platform. These posts were collected using the Pushshift API, encompassing all posts made between December 16, 2008, and January 2, 2021. The dataset consists of 237,074 data points and is divided into two categories: suicide and non-suicide, with an equal distribution of 116,037 data points in each category. On average, the length of each data point in the dataset is approximately 700 words. However, due to resource limitations, our study focuses on a subset of 120,000 randomly selected data points, with an equal distribution of 60,000 data points for both the suicide and non-suicide categories. Notably, the newly constructed dataset does not contain any null values in the text or class values.
Data Preprocessing: After creating our own subset of the original dataset, we proceeded to preprocess it in order to obtain more accurate results. Since the data points were collected from the Reddit social media platform, the dataset may contain irrelevant elements that could negatively affect the model's accuracy. These elements include HTML tags, URLs, numbers, incorrectly encoded text, contractions, punctuation marks, emojis, special characters, and extra spaces. Although these elements do not impact the classification, they can hinder accuracy. Therefore, we removed all of them and converted the text to lowercase characters. Most data points in the dataset consist of a significant number of words, but they also contain common words that have no impact on the classification. These words are known as stopwords. To enhance the effectiveness of model training and reduce computational requirements, we removed these stopwords, which resulted in shorter data point lengths. Before feeding the data into the model, it is necessary to convert the text into numerical representations. We encoded the categories by assigning them numerical values and then converted the categories into categorical data. Additionally, we converted the text input corpus into integer numbers to facilitate its processing by the model. To accomplish this, we utilized the tokenizer class to transform sentences into arrays of numbers based on their frequency. We specifically selected the top 10,000 words, which are the most commonly occurring words, for training our model. The tokenizer only considers words that it recognizes, so we concatenated our training and test data to expand the vocabulary available to the tokenizer. By using the tokenizer, we converted the text of each sentence into a sequence of numbers. As our data comprises sentences of varying lengths, the resulting number sequences also have different lengths. To ensure compatibility with our model, we needed to make all sequences the same length. To achieve this, we employed pad sequences, a technique that adds padding (typically 0) to the sequences so that they match a predefined length. To ensure consistency in our model, we select a post from each sequence, repeating it until every sequence reaches a fixed length. In our case, the sequence length is set to 200 for LSTM and GRU architectures, and 512 for the Bidirectional LSTM architecture. Although we convert each sentence into a numerical sequence, we treat individual words independently. However, these words often have contextual relationships that should be taken into account. To establish these connections, we employ word embedding. Specifically, we utilize the glove-wiki-gigaword-100 embedding, which is trained on Wikipedia data and represents each word as a 100-dimensional vector
